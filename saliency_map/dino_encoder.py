import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class DinoV3Encoder(nn.Module):
    """DINO v3 encoder with slice-by-slice processing for 3D volumes"""
    
    def __init__(self, latent_dim=256, model_size='small', pretrained=True, max_slices=32, slice_sampling='uniform'):
        super().__init__()
        
        # Try multiple approaches to get DINO/ViT models
        self.dino = None
        self.dino_feature_dim = None
        
        # Approach 1: Try torchvision DINO v3 models
        try:
            import torchvision
            print(f"üîç Found torchvision version: {torchvision.__version__}")
            
            # Try different import paths for DINO v3
            dino_models = {}
            
            # Method 1: Direct import
            try:
                from torchvision.models import dinov3_small, dinov3_base, dinov3_large
                dino_models = {'small': dinov3_small, 'base': dinov3_base, 'large': dinov3_large}
                print("‚úÖ Found DINO v3 models via direct import")
            except ImportError:
                pass
            
            # Method 2: Try hub models
            if not dino_models:
                try:
                    import torch
                    dino_models = {
                        'small': lambda pretrained=True: torch.hub.load('facebookresearch/dino', 'dino_vits16', pretrained=pretrained),
                        'base': lambda pretrained=True: torch.hub.load('facebookresearch/dino', 'dino_vitb16', pretrained=pretrained),
                        'large': lambda pretrained=True: torch.hub.load('facebookresearch/dino', 'dino_vitl16', pretrained=pretrained)
                    }
                    print("‚úÖ Found DINO models via torch.hub")
                except Exception:
                    pass
            
            # Method 3: Use Vision Transformer from torchvision as fallback
            if not dino_models:
                from torchvision.models import vit_b_16, vit_b_32, vit_l_16
                dino_models = {
                    'small': lambda pretrained=True: vit_b_16(pretrained=pretrained),
                    'base': lambda pretrained=True: vit_b_16(pretrained=pretrained),
                    'large': lambda pretrained=True: vit_l_16(pretrained=pretrained)
                }
                print("‚ö†Ô∏è  Using ViT models as DINO v3 fallback")
            
            if model_size in dino_models:
                self.dino = dino_models[model_size](pretrained=pretrained)
                
                # Set feature dimensions
                if model_size == 'small':
                    self.dino_feature_dim = 384
                elif model_size == 'base':
                    self.dino_feature_dim = 768
                elif model_size == 'large':
                    self.dino_feature_dim = 1024
                else:
                    self.dino_feature_dim = 768  # default
                
                print(f"‚úÖ Loaded {model_size} model with {self.dino_feature_dim} features")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load DINO models: {e}")
        
        # Fallback: Create a custom ViT-like architecture
        if self.dino is None:
            print("üîß Creating custom Vision Transformer as DINO v3 fallback")
            self.dino = self._create_custom_vit(model_size)
            
            if model_size == 'small':
                self.dino_feature_dim = 384
            elif model_size == 'base':
                self.dino_feature_dim = 768
            else:
                self.dino_feature_dim = 1024
        
        
        print(f"üîß Initializing DINO v3 encoder (size: {model_size}, latent_dim: {latent_dim})")

        self.latent_dim = latent_dim
        self.max_slices = max_slices
        self.slice_sampling = slice_sampling
        self.model_size = model_size
        # Remove classification head
        if hasattr(self.dino, 'head'):
            self.dino.head = nn.Identity()
        elif hasattr(self.dino, 'heads'):
            self.dino.heads = nn.Identity()
        elif hasattr(self.dino, 'classifier'):
            self.dino.classifier = nn.Identity()
        
        print(f"DINO v3 feature dimension: {self.dino_feature_dim}")
        
        # Projection layers
        self.slice_projection = nn.Sequential(
            nn.Linear(self.dino_feature_dim, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        
        # Aggregation across slices - using transformer instead of LSTM
        self.slice_aggregator = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=latent_dim,
                nhead=8,
                dim_feedforward=latent_dim * 2,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2
        )
        
        # Final projection with residual connection
        self.final_projection = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim)
        )
        
        # Normalization layers
        self.slice_norm = nn.LayerNorm(latent_dim)
        self.final_norm = nn.LayerNorm(latent_dim)
        
        # Positional encoding for slices
        self.pos_encoding = self._create_positional_encoding(max_slices, latent_dim)
        
        # Initialize weights
        self._init_weights()
    
    def _create_custom_vit(self, model_size):
        """Create custom Vision Transformer when DINO models aren't available"""
        from torch import nn
        import math
        
        # Model configurations
        configs = {
            'small': {'embed_dim': 384, 'depth': 12, 'num_heads': 6},
            'base': {'embed_dim': 768, 'depth': 12, 'num_heads': 12},
            'large': {'embed_dim': 1024, 'depth': 24, 'num_heads': 16}
        }
        
        config = configs.get(model_size, configs['base'])
        
        class SimpleViT(nn.Module):
            def __init__(self, embed_dim=768, depth=12, num_heads=12, patch_size=16):
                super().__init__()
                self.patch_size = patch_size
                self.embed_dim = embed_dim
                
                # Patch embedding
                self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)
                
                # Positional encoding
                num_patches = (224 // patch_size) ** 2
                self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim) * 0.02)
                self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
                
                # Transformer blocks
                self.blocks = nn.ModuleList([
                    nn.TransformerEncoderLayer(
                        d_model=embed_dim,
                        nhead=num_heads,
                        dim_feedforward=embed_dim * 4,
                        dropout=0.1,
                        batch_first=True
                    ) for _ in range(depth)
                ])
                
                self.norm = nn.LayerNorm(embed_dim)
                
            def forward(self, x):
                B, C, H, W = x.shape
                
                # Patch embedding
                x = self.patch_embed(x)  # B, embed_dim, H//patch_size, W//patch_size
                x = x.flatten(2).transpose(1, 2)  # B, num_patches, embed_dim
                
                # Add class token
                cls_tokens = self.cls_token.expand(B, -1, -1)
                x = torch.cat([cls_tokens, x], dim=1)
                
                # Add positional encoding
                x = x + self.pos_embed
                
                # Apply transformer blocks
                for block in self.blocks:
                    x = block(x)
                
                # Layer norm and return class token
                x = self.norm(x)
                return x[:, 0]  # Return CLS token
        
        return SimpleViT(**config)
    
    def _create_positional_encoding(self, max_len, d_model):
        """Create sinusoidal positional encoding"""
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return nn.Parameter(pe, requires_grad=False)
    
    def _init_weights(self):
        """Initialize the projection layers"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def _sample_slices(self, volume_3d):
        """Sample slice indices based on sampling strategy"""
        batch_size, _, depth, height, width = volume_3d.shape
        
        if self.slice_sampling == 'all':
            slice_indices = torch.arange(depth)
        elif self.slice_sampling == 'uniform':
            if depth > self.max_slices:
                slice_indices = torch.linspace(0, depth-1, self.max_slices).long()
            else:
                slice_indices = torch.arange(depth)
        elif self.slice_sampling == 'adaptive':
            # Adaptive sampling: more slices in the center
            center = depth // 2
            indices = torch.linspace(center - self.max_slices//2, 
                                   center + self.max_slices//2, 
                                   self.max_slices).clamp(0, depth-1).long()
            slice_indices = torch.unique(indices)
        elif self.slice_sampling == 'random':
            # Random sampling
            if depth > self.max_slices:
                slice_indices = torch.randperm(depth)[:self.max_slices].sort()[0]
            else:
                slice_indices = torch.arange(depth)
        else:
            raise ValueError(f"Unknown slice_sampling method: {self.slice_sampling}")
        
        return slice_indices
    
    def _preprocess_slice(self, slice_2d):
        """Preprocess 2D slice for DINO v3 input"""
        # Ensure positive values and normalize to [0, 1]
        slice_2d = torch.clamp(slice_2d, min=0)
        slice_min = slice_2d.view(slice_2d.shape[0], -1).min(dim=1, keepdim=True)[0].unsqueeze(-1).unsqueeze(-1)
        slice_max = slice_2d.view(slice_2d.shape[0], -1).max(dim=1, keepdim=True)[0].unsqueeze(-1).unsqueeze(-1)
        slice_2d = (slice_2d - slice_min) / (slice_max - slice_min + 1e-8)
        
        # Resize to 224x224 for DINO v3
        slice_2d = F.interpolate(
            slice_2d, size=(224, 224), 
            mode='bilinear', align_corners=False
        )
        
        # Convert to 3-channel (RGB) - DINO v3 expects RGB
        slice_2d = slice_2d.repeat(1, 3, 1, 1)
        
        # Normalize to ImageNet stats (DINO v3 expects this)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(slice_2d.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(slice_2d.device)
        slice_2d = (slice_2d - mean) / std
        
        return slice_2d
    
    def forward(self, volume_3d):
        """Forward pass through DINO v3 encoder"""
        # volume_3d: (batch, 1, D, H, W)
        batch_size, _, depth, height, width = volume_3d.shape
        device = volume_3d.device
        
        # Sample slice indices
        slice_indices = self._sample_slices(volume_3d)
        
        slice_features = []
        
        # Process each slice
        for idx in slice_indices:
            # Extract slice: (batch, 1, H, W)
            slice_2d = volume_3d[:, :, idx, :, :]
            
            # Preprocess for DINO v3
            slice_2d = self._preprocess_slice(slice_2d)
            
            # Get DINO v3 features
            try:
                with torch.no_grad():
                    features = self.dino(slice_2d)
                    
                    # Handle different output formats
                    if isinstance(features, dict):
                        # Handle DINO hub models that return dictionaries
                        features = features.get('x_norm_clstoken', 
                                               features.get('cls_token', 
                                                           features.get('x', features)))
                    elif isinstance(features, tuple):
                        # Handle models that return tuples
                        features = features[0]
                    
                    # Ensure correct shape
                    if features.dim() > 2:
                        features = features.view(features.shape[0], -1)  # Flatten if needed
                    
                    # Handle different feature dimensions
                    if features.shape[-1] != self.dino_feature_dim:
                        print(f"‚ö†Ô∏è  Feature dimension mismatch: expected {self.dino_feature_dim}, got {features.shape[-1]}")
                        # Adapt the projection layer if needed
                        if not hasattr(self, '_adapted_projection'):
                            self.slice_projection = nn.Sequential(
                                nn.Linear(features.shape[-1], self.dino_feature_dim),
                                nn.ReLU(),
                                nn.Linear(self.dino_feature_dim, self.latent_dim),
                                nn.ReLU(),
                                nn.Dropout(0.1)
                            ).to(device)
                            self._adapted_projection = True
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error in DINO forward pass: {e}")
                # Use fallback features
                features = torch.randn(batch_size, self.dino_feature_dim, device=device)
            
            # Project to latent dimension
            slice_latent = self.slice_projection(features.float().detach())
            slice_latent = self.slice_norm(slice_latent)
            slice_features.append(slice_latent)
        
        # Stack slice features: (batch, num_slices, latent_dim)
        if len(slice_features) == 0:
            # Fallback if no slices were processed
            print("‚ö†Ô∏è  No slices processed, returning zero tensor")
            return torch.zeros(batch_size, self.latent_dim, device=device)
        
        volume_features = torch.stack(slice_features, dim=1)
        num_slices = volume_features.shape[1]
        
        # Add positional encoding
        if num_slices <= self.pos_encoding.shape[0]:
            pos_enc = self.pos_encoding[:num_slices].unsqueeze(0).expand(batch_size, -1, -1)
            volume_features = volume_features + pos_enc.to(device)
        
        # Transformer aggregation
        aggregated_features = self.slice_aggregator(volume_features)
        
        # Global average pooling over slices
        final_features = aggregated_features.mean(dim=1)
        
        # Final projection with residual connection
        projected = self.final_projection(final_features)
        final_features = final_features + projected  # Residual connection
        
        # Final normalization
        final_features = self.final_norm(final_features)
        
        return final_features


# Update the create_encoder factory function
def create_encoder(encoder_type='simple_cnn', latent_dim=256, **kwargs):
    """Factory function to create encoders"""
    
    if encoder_type == 'simple_cnn':
        from models import Simple3DCNNEncoder
        return Simple3DCNNEncoder(latent_dim=latent_dim, **kwargs)
    
    elif encoder_type == 'timm_vit':
        from models import TimmViTEncoder
        return TimmViTEncoder(latent_dim=latent_dim, **kwargs)
    
    elif encoder_type == 'resnet3d':
        from models import ResNet3DEncoder
        return ResNet3DEncoder(latent_dim=latent_dim, **kwargs)
    
    elif encoder_type == 'hybrid':
        from models import LightweightHybridEncoder
        return LightweightHybridEncoder(latent_dim=latent_dim, **kwargs)
    
    elif encoder_type == 'dino_v3':
        return DinoV3Encoder(latent_dim=latent_dim, **kwargs)
    
    else:
        raise ValueError(f"Unknown encoder type: {encoder_type}")


# Usage example and testing
if __name__ == "__main__":
    def test_dino_encoder():
        """Test DINO v3 encoder with various configurations"""
        
        print("üß™ Testing DINO v3 Encoder Implementation")
        print("=" * 50)
        
        # Test different DINO v3 configurations
        configs = [
            {'model_size': 'small', 'latent_dim': 256, 'max_slices': 16, 'slice_sampling': 'uniform'},
            {'model_size': 'base', 'latent_dim': 512, 'max_slices': 24, 'slice_sampling': 'adaptive'},
        ]
        
        # Create test input - smaller volume for faster testing
        dummy_input = torch.randn(1, 1, 32, 64, 64)  # Small volume for testing
        print(f"üìä Test input shape: {dummy_input.shape}")
        
        for i, config in enumerate(configs):
            print(f"\nüîß Test {i+1}: {config['model_size']} model")
            print(f"   Config: {config}")
            
            try:
                # Create encoder
                encoder = DinoV3Encoder(**config)
                
                # Test forward pass
                with torch.no_grad():
                    output = encoder(dummy_input)
                    
                print(f"‚úÖ Success! Input {dummy_input.shape} -> Output {output.shape}")
                
                # Verify output shape
                expected_shape = (dummy_input.shape[0], config['latent_dim'])
                assert output.shape == expected_shape, f"Expected {expected_shape}, got {output.shape}"
                print(f"‚úÖ Output shape verified: {output.shape}")
                
                # Test with different slice sampling methods
                for sampling in ['uniform', 'adaptive', 'random']:
                    encoder.slice_sampling = sampling
                    with torch.no_grad():
                        test_output = encoder(dummy_input)
                    print(f"‚úÖ {sampling} sampling: {test_output.shape}")
                
                print(f"‚úÖ All tests passed for {config['model_size']} model!")
                
            except Exception as e:
                print(f"‚ùå Error with {config['model_size']} model: {e}")
                print(f"   This might be due to missing DINO models, but encoder should still work with fallback")
        
        # Test the create_encoder factory function
        print(f"\nüè≠ Testing factory function...")
        try:
            encoder = create_encoder(encoder_type='dino_v3', latent_dim=256, model_size='small')
            with torch.no_grad():
                output = encoder(dummy_input)
            print(f"‚úÖ Factory function test passed: {output.shape}")
        except Exception as e:
            print(f"‚ùå Factory function test failed: {e}")
        
        print(f"\nüéâ DINO v3 encoder testing completed!")
        
    # Run the test
    test_dino_encoder()